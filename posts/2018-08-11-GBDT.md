---
title: GBDT 介绍
author: 张凯
tags: machine-learning
---

本文介绍了 GBDT（Gradient Boosting Decision Tree) 方法[@friedman2001elements]。

<!--more-->

## 损失函数

均方误差
: $$L(y, f(x)) = (y-f(x))^2$$

绝对值误差
: $$L(y, f(x)) = \lvert y-f(x)\rvert$$

Huber 损失
: $$L(y, f(x)) =
\begin{cases}
  [y - f(x)]^2 & \textrm{for } |y - f(x)| \le \delta,\\
  2\delta|y - f(x)| - \delta^2 & \textrm{otherwise.}
\end{cases}$$

| 名称       | 优点                 | 缺点                 |
|------------|----------------------|----------------------|
| 均方误差   | 直观，容易生成决策树 | 对错误敏感，不够健壮 |
| 绝对值误差 | 健壮                 | 不易生成决策树       |
| Huber 损失 | 健壮                 | 不易生成决策树       |

: 不同损失函数的优缺点 {#tbl:lossCompare}

## 决策树

决策树概念简单、功能强大，而且泛化能力好；但是不一定容易生成。特别地，损失函数为
均方误差的决策树称为回归树，可以用贪心算法递归生成。

## 梯度下降法

梯度下降法作为优化方法比决策树适用范围更广，可以优化绝对值误差损失函数和 Huber
损失函数；但容易产生过拟合，泛化能力差。

## GBDT

GBDT 是一种提升方法，先用较弱的模型作为输出，然后在弱模型的基础上向更优的方向迭
代。为了模型的健壮性，我们需要选择健壮的损失函数；此时我们只能使用梯度下降法来进
行迭代优化；为了得到更好的泛化能力，每次迭代时用回归树来近似梯度，并以贪婪策略选
择学习速率。这样，我们就推导出了 GBDT[@friedman2001elements]：

1. 初始化：$f_0(x) = \textrm{arg}\min_{\gamma}\sum_{i=1}^N L(y_i, \gamma)$
2. 对 $m = 1 \to M$：
    1. $r_{im} = -\left[\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}\right]_{f=f_{m-1}}$
       其中，$i = 1, 2, \dots, N$；
    2. 求 $r_{im}$ 的回归树，记所得终端区域为 $R_{jm}, j = 1, 2, \dots, J_m$；
    3. 对 $j = 1, 2, \dots, J_m$，计算
       $$\gamma_{jm} = \textrm{arg}\min_{\gamma}\sum_{x_i\in R_{jm}}L(y_i,f_{m-1}(x_i)+\gamma)$$
    4. 更新 $f_m(x) = f_{m-1}(x) + \sum_{j=1}^{J_m}\gamma_{jm}I(x\in R_{jm})$
3. 输出 $\hat{f}(x) = f_M(x)$

## 参考文献
